{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMTOOLS -- Preprocessing\n",
    "This notebook preprocesses images for prediction using a DNN trained with Uni-EM. It opens a series of .tiff or .png files in `path_input` and applies CLAHE (Contrast limited adaptive histogram equalization), which enhances the local contrast of images. It then re-saves them as RGB .png files to `path_results`.\n",
    "\n",
    "**Author:** Philip Ruthig, Paul Flechsig Institute, Center of Neuropathology and Brain Research Leipzig\n",
    "\n",
    "**Contact:** philip.ruthig@medizin.uni-leipzig.de // philip.ruthig@gmail.com\n",
    "\n",
    "**Publication:**\n",
    "Please contact me if you want to use this code for any publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import skimage\n",
    "from skimage.transform import downscale_local_mean\n",
    "import tqdm\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def better_chunk_generator(img_shape, overlap, resize_chunks=False):\n",
    "    sizes = [2048, 1024, 512, 256]  # list of possible chunk sizes\n",
    "\n",
    "    for size in sizes:\n",
    "        if img_shape[0] >= size and img_shape[1] >= size:\n",
    "            chunk_size = (size, size)\n",
    "            break\n",
    "\n",
    "    y_start = 0\n",
    "    \n",
    "    while y_start < img_shape[0]:\n",
    "        y_end = y_start + chunk_size[0]\n",
    "        if y_end > img_shape[0]:\n",
    "            break\n",
    "        \n",
    "        x_start = 0\n",
    "        while x_start < img_shape[1]:\n",
    "            x_end = x_start + chunk_size[1]\n",
    "            if x_end > img_shape[1]:\n",
    "                break\n",
    "            \n",
    "            yield (y_start, y_end, x_start, x_end)\n",
    "            x_start += chunk_size[1] - overlap\n",
    "        \n",
    "        y_start += chunk_size[0] - overlap\n",
    "\n",
    "\n",
    "def chunk_generator(img_shape, chunk_size, overlap, resize_chunks=False):\n",
    "    y_start = 0\n",
    "    \n",
    "    while y_start < img_shape[0]:\n",
    "        y_end = y_start + chunk_size[0]\n",
    "        if y_end > img_shape[0]:\n",
    "            break\n",
    "        \n",
    "        x_start = 0\n",
    "        while x_start < img_shape[1]:\n",
    "            x_end = x_start + chunk_size[1]\n",
    "            if x_end > img_shape[1]:\n",
    "                break\n",
    "            \n",
    "            yield (y_start, y_end, x_start, x_end)\n",
    "            x_start += chunk_size[1] - overlap\n",
    "        \n",
    "        y_start += chunk_size[0] - overlap\n",
    "\n",
    "def crop_background(img, background_threshold):\n",
    "    '''\n",
    "    Returns the largest rectangular region within your 2d image that does not contain background.\n",
    "    Useful if your images contain grid shadows and you want to exclude them from your analysis.\n",
    "    '''\n",
    "    rows, cols = img.shape\n",
    "    max_area = 0\n",
    "    max_top = max_left = max_bottom = max_right = 0\n",
    "\n",
    "    # Loop through each element in the 2D array (image)\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            # Check if the element meets the background threshold condition\n",
    "            if img[i, j] >= background_threshold:\n",
    "                top = bottom = i\n",
    "                left = right = j\n",
    "\n",
    "                # Expand the region vertically until the background threshold condition is not met\n",
    "                while bottom < rows and img[bottom, j] >= background_threshold:\n",
    "                    bottom += 1\n",
    "\n",
    "                # Expand the region horizontally until the background threshold condition is not met\n",
    "                while right < cols and np.all(img[i:bottom, right] >= background_threshold):\n",
    "                    right += 1\n",
    "\n",
    "                # Calculate the area of the current rectangular region\n",
    "                area = (bottom - i) * (right - j)\n",
    "\n",
    "                # Update the maximum area and the coordinates of the maximum rectangular region\n",
    "                if area > max_area:\n",
    "                    max_area = area\n",
    "                    max_top, max_left, max_bottom, max_right = i, j, bottom, right\n",
    "\n",
    "    # Return the largest rectangular region based on the maximum coordinates\n",
    "    return img[max_top:max_bottom, max_left:max_right]\n",
    "\n",
    "def remove_whitespaces(string):\n",
    "    return \"\".join(string.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user inputs\n",
    "path_input = r\"0_raw\\\\\"\n",
    "path_results = r\"1_preprocessed\\\\\"\n",
    "ds = 4 # each axis of the image is downsampled with this factor.\n",
    "bg_crop = False # set to true if you want your data has black edges which you want to be cropped\n",
    "background_threshold = 100 # intensity threshold for background - if your image has dark background that you would like to crop, define a threshold for it here. Everything below that threshold is cropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_images = [f for f in listdir(path_input) if isfile(join(path_input, f)) and f != \".gitkeep\"]\n",
    "for i in tqdm.tqdm(range(len(path_images))):\n",
    "    # print(\"Preprocessing Images \" + str(i))\n",
    "    if path_images[i].endswith(\".tif\") or path_images[i].endswith(\".tiff\") or path_images[i].endswith(\".TIF\") or path_images[i].endswith(\".TIFF\"):\n",
    "        test_img = tf.imread(path_input + str(path_images[i])) # use this for tiff\n",
    "    elif path_images[i].endswith(\".png\") or path_images[i].endswith(\".PNG\"):\n",
    "        test_img = cv2.imread(path_input + str(path_images[i]),-1) # use this for png \n",
    "    else:\n",
    "        print('Input file format not supported. Use .png or .tif.')\n",
    "        break\n",
    "    if bg_crop == True:\n",
    "        if background_threshold < test_img.min():\n",
    "            print('cropping background..')\n",
    "            test_img = crop_background(test_img,background_threshold)\n",
    "\n",
    "    # if image is small\n",
    "    if np.array(test_img.shape[0:1]).min() < 2048*ds:\n",
    "        coord_list = []\n",
    "        for y_start, y_end, x_start, x_end in chunk_generator(test_img.shape, (1024*ds,1024*ds),0):\n",
    "            coord_list.append((y_start,y_end,x_start,x_end))\n",
    "            for zyx in tqdm.tqdm(coord_list): #iterate through image, \n",
    "                test_img_clahe = skimage.exposure.equalize_adapthist(test_img[zyx[0]:zyx[1],zyx[2]:zyx[3]],clip_limit=0.01,kernel_size=127)\n",
    "                test_img_downscaled = downscale_local_mean(test_img_clahe, ds)\n",
    "                test_img_rgb_png = cv2.merge((downscale_local_mean(test_img_downscaled,1),  #R\n",
    "                                            downscale_local_mean(test_img_downscaled,1),    #G\n",
    "                                            downscale_local_mean(test_img_downscaled,1)))   #B\n",
    "                skimage.io.imsave(path_results + path_images[i][:-4] +  remove_whitespaces(str(zyx)) + \".png\", (test_img_rgb_png*255).astype('uint8'))\n",
    "            continue\n",
    "    \n",
    "    # if image is big (at least 2048*ds x 2048*ds shape)\n",
    "    coord_list = []\n",
    "    for y_start, y_end, x_start, x_end in chunk_generator(test_img.shape, (2048*ds,2048*ds),0):\n",
    "        coord_list.append((y_start,y_end,x_start,x_end))\n",
    "    for zyx in tqdm.tqdm(coord_list): #iterate through image, \n",
    "        test_img_clahe = skimage.exposure.equalize_adapthist(test_img[zyx[0]:zyx[1],zyx[2]:zyx[3]],clip_limit=0.01,kernel_size=127)\n",
    "        test_img_downscaled = downscale_local_mean(test_img_clahe, ds)\n",
    "        test_img_rgb_png = cv2.merge((downscale_local_mean(test_img_downscaled,1),  #R\n",
    "                                    downscale_local_mean(test_img_downscaled,1),    #G\n",
    "                                    downscale_local_mean(test_img_downscaled,1)))   #B\n",
    "        skimage.io.imsave(path_results + path_images[i][:-4] + remove_whitespaces(str(zyx)) +  \".png\", (test_img_rgb_png*255).astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
